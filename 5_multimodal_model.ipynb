{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Building the Multimodal Model\n",
    "\n",
    "We incorporate our learnings from part 4 into building a Multimodal Model with different processing pathways for the various modalities, and then merge their processed features for the final classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.api._v2.keras as keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Dense, Input, Activation, Lambda, BatchNormalization, Conv1D, SpatialDropout1D, add, GlobalAveragePooling1D, LSTM, Dense, concatenate, TimeDistributed, Bidirectional, Dropout, Embedding, Attention, MultiHeadAttention, LayerNormalization, Flatten, Concatenate\n",
    "from keras.activations import sigmoid\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import tensorflow as tf\n",
    "from data_load import load_data\n",
    "from basemodel import BaseModel\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  120\n",
      "Number of testing samples:  31\n",
      "Max length of sequences:  110\n",
      "\n",
      "No. of samples per class: {'neu': 1708, 'fru': 1849, 'ang': 1103, 'sad': 1084, 'exc': 1041, 'hap': 648}\n",
      "\n",
      "Train text shape: 120 samples, 110 timesteps, 100 features\n",
      "Train audio shape: 120 samples, 110 timesteps, 100 features\n",
      "Train visual shape: 120 samples, 110 timesteps, 512 features\n",
      "\n",
      "Test text shape: 31 samples, 110 timesteps, 100 features\n",
      "Test audio shape: 31 samples, 110 timesteps, 100 features\n",
      "Test visual shape: 31 samples, 110 timesteps, 512 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data, train_label, test_data, test_label, train_text, train_audio, train_visual, test_text, test_audio, test_visual = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block and Cross-Modal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs, inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class CrossModalAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attn = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "    \n",
    "    def call(self, query, key_value):\n",
    "        attn_output = self.multi_head_attn(query, key_value, key_value)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Multimodal Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, num_classes=10):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        # Transformer block for processing video features\n",
    "        self.video_transformer = TransformerBlock(d_model=512, num_heads=8, dff=dff, rate=rate)\n",
    "        \n",
    "        # Reducing dimensionality of processed video features to align with audio and text features\n",
    "        self.dimensionality_reduction = Dense(d_model)\n",
    "        \n",
    "        # Text feature processing using bi-LSTM\n",
    "        self.text_lstm = Bidirectional(LSTM(50, return_sequences=True))\n",
    "\n",
    "        # Audio feature processing using 1D CNN with padding\n",
    "        self.audio_cnn = Conv1D(filters=100, kernel_size=3, activation='relu', padding='same')\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Final processing and classification layers\n",
    "        self.concat = Concatenate()\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        audio_features, text_features, video_features = inputs\n",
    "        \n",
    "        # Process video features and reduce dimensionality\n",
    "        video_features_transformed = self.video_transformer(video_features, training=training)\n",
    "        video_features_reduced = self.dimensionality_reduction(video_features_transformed)\n",
    "\n",
    "        # Process text features\n",
    "        text_features = self.text_lstm(text_features)\n",
    "\n",
    "        # Process audio features\n",
    "        audio_features = self.audio_cnn(audio_features)\n",
    "        \n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(text_features, audio_features)\n",
    "        text_video_attn = self.text_video_attention(text_features, video_features_reduced)\n",
    "        \n",
    "        # Combine features from both attention mechanisms\n",
    "        combined_features = self.concat([text_features, text_audio_attn, text_video_attn])\n",
    "        outputs = self.classifier(combined_features)\n",
    "\n",
    "        # Return the final prediction\n",
    "        return outputs\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        # Return a dictionary of metrics\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 22:50:05.545646: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2024-03-31 22:50:05.545690: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-03-31 22:50:05.545709: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-03-31 22:50:05.545748: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:303] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-03-31 22:50:05.545763: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:269] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2024-03-31 22:50:07.255855: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 22:50:07.671922: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 22:50:07.690259: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 22:50:08.180784: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 22:50:08.214045: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 4s 406ms/step - loss: 1.8638 - accuracy: 0.3392\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 282ms/step - loss: 1.2259 - accuracy: 0.6286\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 251ms/step - loss: 0.9728 - accuracy: 0.7845\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 246ms/step - loss: 0.7397 - accuracy: 0.6864\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 244ms/step - loss: 0.5841 - accuracy: 0.8660\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 255ms/step - loss: 0.4581 - accuracy: 0.8821\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 255ms/step - loss: 0.3431 - accuracy: 0.9314\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 246ms/step - loss: 0.2654 - accuracy: 0.9491\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 241ms/step - loss: 0.2168 - accuracy: 0.9503\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 246ms/step - loss: 0.1802 - accuracy: 0.9567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 22:50:19.523416: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 22:50:19.652896: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 22:50:19.676680: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 645ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7926686217008798,\n",
       " 'precision': 0.7005309195404089,\n",
       " 'recall': 0.6409352371358086,\n",
       " 'f1_score': 0.6609940912578373}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "multimodal_model = MultimodalModel(d_model=100, num_heads=4, dff=2048, rate=0.1, num_classes=6)\n",
    "\n",
    "# Compile the model\n",
    "multimodal_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodal_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodal_model.evaluate([test_audio, test_text, test_visual], test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAttentionModel(Model):\n",
    "    def __init__(self, text_input_shape, audio_input_shape, video_input_shape, num_classes, d_model=128, num_heads=4):\n",
    "        super(MultimodalAttentionModel, self).__init__()\n",
    "        \n",
    "        # Text Pathway - Bi-LSTM with Attention\n",
    "        self.text_attention = MultiHeadAttention(num_heads=4, key_dim=d_model // num_heads)\n",
    "        self.text_bi_lstm1 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        self.text_bi_lstm2 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        self.text_dropout = Dropout(0.5)\n",
    "        self.text_dense = Dense(64, activation='relu')\n",
    "        \n",
    "        # Audio Pathway - CNN with Attention\n",
    "        self.audio_attention = MultiHeadAttention(num_heads=8, key_dim=d_model // num_heads)\n",
    "        self.audio_conv1 = Conv1D(64, kernel_size=10, activation='relu', padding='same')\n",
    "        self.audio_conv2 = Conv1D(64, kernel_size=9, activation='relu', padding='same')\n",
    "        self.audio_conv3 = Conv1D(64, kernel_size=8, activation='relu', padding='same')\n",
    "        self.audio_dense = Dense(64, activation='relu')\n",
    "        \n",
    "        # Video Pathway - Similar to Audio\n",
    "        self.video_attention = MultiHeadAttention(num_heads=8, key_dim=d_model // num_heads)\n",
    "        self.video_conv1 = Conv1D(64, kernel_size=10, activation='relu', padding='same')\n",
    "        self.video_conv2 = Conv1D(64, kernel_size=9, activation='relu', padding='same')\n",
    "        self.video_conv3 = Conv1D(64, kernel_size=8, activation='relu', padding='same')\n",
    "        self.video_dense = Dense(64, activation='relu')\n",
    "        \n",
    "        # Integration and Classification\n",
    "        self.concat = Concatenate()\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.final_classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Final processing and classification layers\n",
    "        self.concat = Concatenate()\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        text_inputs, audio_inputs, video_inputs = inputs\n",
    "        \n",
    "        # Text Pathway\n",
    "        x_text = self.text_attention(text_inputs, text_inputs)\n",
    "        x_text = self.text_bi_lstm1(x_text)\n",
    "        x_text = self.text_bi_lstm2(x_text)\n",
    "        x_text = self.text_dropout(x_text, training=training)\n",
    "        x_text = self.text_dense(x_text)\n",
    "        \n",
    "        # Audio Pathway\n",
    "        x_audio = self.audio_attention(audio_inputs, audio_inputs)\n",
    "        x_audio = self.audio_conv1(x_audio)\n",
    "        x_audio = self.audio_conv2(x_audio)\n",
    "        x_audio = self.audio_conv3(x_audio)\n",
    "        x_audio = self.audio_dense(x_audio)\n",
    "        \n",
    "        # Video Pathway\n",
    "        x_video = self.video_attention(video_inputs, video_inputs)\n",
    "        x_video = self.video_conv1(x_video)\n",
    "        x_video = self.video_conv2(x_video)\n",
    "        x_video = self.video_conv3(x_video)\n",
    "        x_video = self.video_dense(x_video)\n",
    "\n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(x_text, x_audio)\n",
    "        text_video_attn = self.text_video_attention(x_text, x_video)\n",
    "        \n",
    "        # Combine features from both attention mechanisms\n",
    "        combined_features = self.concat([x_text, text_audio_attn, text_video_attn])\n",
    "        outputs = self.final_classifier(combined_features)\n",
    "\n",
    "        # Return the final prediction\n",
    "        return outputs\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        self.test_metrics =  {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'cm': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "        print(\"Test Metrics:\")\n",
    "        print(f\"Accuracy: {self.test_metrics['accuracy']:.4f}\")\n",
    "        # print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "        # print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {self.test_metrics['f1_score']:.4f}\")\n",
    "       \n",
    "    \n",
    "    def print_metrics(self):\n",
    "        print(\"Test Metrics:\")\n",
    "        print(f\"Accuracy: {self.test_metrics['accuracy']:.4f}\")\n",
    "        # print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        # print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {self.test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "        class_labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(self.test_metrics['cm'], annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 23:09:22.601278: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:23.337751: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:23.368610: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:23.546741: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:23.562206: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:23.934354: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:23.979518: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:24.224675: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:24.247903: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 6s 407ms/step - loss: 1.7728 - accuracy: 0.4505\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 218ms/step - loss: 1.3744 - accuracy: 0.5980\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 214ms/step - loss: 1.2748 - accuracy: 0.5980\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 218ms/step - loss: 1.1637 - accuracy: 0.5986\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 211ms/step - loss: 1.0767 - accuracy: 0.6012\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 267ms/step - loss: 1.0344 - accuracy: 0.5858\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 169ms/step - loss: 0.9772 - accuracy: 0.6447\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 199ms/step - loss: 0.8301 - accuracy: 0.6995\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 188ms/step - loss: 0.7692 - accuracy: 0.6953\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 183ms/step - loss: 0.7102 - accuracy: 0.7319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-31 23:09:33.876496: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:34.159294: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:34.185118: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:34.366837: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-31 23:09:34.379276: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "Test Metrics:\n",
      "Accuracy: 0.7079\n",
      "F1 Score: 0.5066\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "multimodal_model = MultimodalAttentionModel(text_input_shape=(train_text.shape[1], train_text.shape[2]), audio_input_shape=(train_audio.shape[1], train_audio.shape[2]), video_input_shape=(train_visual.shape[1], train_visual.shape[2]), num_classes=6)\n",
    "\n",
    "# Compile the model\n",
    "multimodal_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodal_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodal_model.evaluate([test_audio, test_text, test_visual], test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
