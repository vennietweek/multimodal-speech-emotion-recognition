{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.api._v2.keras as keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.layers import Layer, Dense, Input, Activation, Lambda, BatchNormalization, Conv1D, SpatialDropout1D, add, GlobalAveragePooling1D, LSTM, Dense, concatenate, TimeDistributed, Bidirectional, Dropout, Embedding, Attention, MultiHeadAttention, LayerNormalization, Flatten, Concatenate\n",
    "from keras.activations import sigmoid\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import tensorflow as tf\n",
    "from data_load import load_data\n",
    "from basemodel import BaseModel\n",
    "from train import cross_val_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples:  120\n",
      "Number of testing samples:  31\n",
      "Max length of sequences:  110\n",
      "\n",
      "Train text shape: 120 samples, 110 timesteps, 100 features\n",
      "Train audio shape: 120 samples, 110 timesteps, 100 features\n",
      "Train visual shape: 120 samples, 110 timesteps, 512 features\n",
      "\n",
      "Test text shape: 31 samples, 110 timesteps, 100 features\n",
      "Test audio shape: 31 samples, 110 timesteps, 100 features\n",
      "Test visual shape: 31 samples, 110 timesteps, 512 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data, train_label, test_data, test_label, train_text, train_audio, train_visual, test_text, test_audio, test_visual = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=int(d_model/num_heads))\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs, inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class CrossModalAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attn = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "    \n",
    "    def call(self, query, key_value):\n",
    "        attn_output = self.multi_head_attn(query, key_value, key_value)\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class MultimodalModel(Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, num_classes=10):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        # Transformer block for processing video features\n",
    "        self.video_transformer = TransformerBlock(d_model=512, num_heads=8, dff=dff, rate=rate)\n",
    "        \n",
    "        # Reducing dimensionality of processed video features to align with audio and text features\n",
    "        self.dimensionality_reduction = Dense(d_model)\n",
    "        \n",
    "        # Text feature processing using bi-LSTM\n",
    "        self.text_lstm = Bidirectional(LSTM(d_model, return_sequences=True))\n",
    "\n",
    "        # Audio feature processing using 1D CNN\n",
    "        self.audio_cnn = Conv1D(filters=d_model, kernel_size=3, activation='relu')\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Final processing and classification layers\n",
    "        self.concat = Concatenate()\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        audio_features, text_features, video_features = inputs\n",
    "        \n",
    "        # Process video features and reduce dimensionality\n",
    "        video_features_transformed = self.video_transformer(video_features, training=training)\n",
    "        video_features_reduced = self.dimensionality_reduction(video_features_transformed)\n",
    "\n",
    "        # Process text features\n",
    "        text_features = self.text_lstm(text_features)\n",
    "        print(text_features.shape)\n",
    "\n",
    "        # Process audio features\n",
    "        audio_features = self.audio_cnn(audio_features)\n",
    "        print(audio_features.shape)\n",
    "        \n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(text_features, audio_features)\n",
    "        text_video_attn = self.text_video_attention(text_features, video_features_reduced)\n",
    "        \n",
    "        # Combine features from both attention mechanisms\n",
    "        combined_features = self.concat([text_features, text_audio_attn, text_video_attn])\n",
    "        print(combined_features.shape)\n",
    "        outputs = self.classifier(combined_features)\n",
    "\n",
    "        # Return the final prediction\n",
    "        return outputs\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        # Return a dictionary of metrics\n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "(None, 110, 600)\n",
      "(None, 110, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 16:48:15.442285: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-30 16:48:15.851566: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-30 16:48:15.956300: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-30 16:48:16.842938: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-30 16:48:16.878574: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 5s 542ms/step - loss: 1.3428 - accuracy: 0.4675\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 1s 295ms/step - loss: 0.9369 - accuracy: 0.7101\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 1s 287ms/step - loss: 0.4527 - accuracy: 0.9301\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 1s 263ms/step - loss: 0.2040 - accuracy: 0.9558\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 1s 272ms/step - loss: 0.1569 - accuracy: 0.9630\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 1s 265ms/step - loss: 0.1304 - accuracy: 0.9635\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 1s 285ms/step - loss: 0.1162 - accuracy: 0.9654\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 1s 246ms/step - loss: 0.1062 - accuracy: 0.9669\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 1s 250ms/step - loss: 0.0988 - accuracy: 0.9686\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 1s 245ms/step - loss: 0.0937 - accuracy: 0.9690\n",
      "(None, 110, 600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 16:48:29.375970: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-30 16:48:29.502123: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2024-03-30 16:48:29.526062: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 825ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7917888563049853,\n",
       " 'precision': 0.6727422557231058,\n",
       " 'recall': 0.6462977881366591,\n",
       " 'f1_score': 0.6498058877653472}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example instantiation and compilation\n",
    "multimodal_model = MultimodalModel(d_model=100, num_heads=4, dff=2048, rate=0.1, num_classes=6)\n",
    "\n",
    "# Compile the model\n",
    "multimodal_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodal_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodal_model.evaluate([test_audio, test_text, test_visual], test_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
