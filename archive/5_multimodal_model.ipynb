{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Building the Multimodal Model\n",
    "\n",
    "We incorporate our learnings from part 4 into building a Multimodal Model with different processing pathways for the various modalities, and then merge their processed features for the final classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.api._v2.keras as keras\n",
    "import keras.backend as K\n",
    "from keras.optimizers.legacy import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.layers import Masking, Layer, Dense, Input, Activation, Lambda, BatchNormalization, Conv1D, SpatialDropout1D, Add, GlobalAveragePooling1D, LSTM, Dense, concatenate, TimeDistributed, Bidirectional, Dropout, Embedding, Attention, MultiHeadAttention, LayerNormalization, Flatten, Concatenate\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import tensorflow as tf\n",
    "from data_utils import load_data\n",
    "from model_utils import *\n",
    "import tensorflow as tf\n",
    "from collections import Counter\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of sequences:  110\n",
      "\n",
      "Number of training samples: 120\n",
      "Number of testing samples: 31\n",
      "\n",
      "Train text shape: 120 samples, 110 timesteps, 100 features\n",
      "Train audio shape: 120 samples, 110 timesteps, 100 features\n",
      "Train visual shape: 120 samples, 110 timesteps, 512 features\n",
      "\n",
      "Test text shape: 31 samples, 110 timesteps, 100 features\n",
      "Test audio shape: 31 samples, 110 timesteps, 100 features\n",
      "Test visual shape: 31 samples, 110 timesteps, 512 features\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data, train_label, train_mask, test_data, test_label, test_mask, train_text, train_audio, train_visual, test_text, test_audio, test_visual = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = tf.convert_to_tensor(train_mask, dtype=tf.float32)\n",
    "test_mask = tf.convert_to_tensor(test_mask, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block and Cross-Modal Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs, inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2\n",
    "\n",
    "class CrossModalAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.multi_head_attn = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "    \n",
    "    def call(self, query, key_value):\n",
    "        attn_output = self.multi_head_attn(query, key_value, key_value)\n",
    "        return attn_output\n",
    "    \n",
    "def masked_categorical_crossentropy(y_true, y_pred, mask):\n",
    "    # Calculate the categorical crossentropy loss\n",
    "    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Mask the loss using the pre-computed mask\n",
    "    loss *= mask\n",
    "    \n",
    "    # Average the loss, ignoring the padded timesteps\n",
    "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal model with cross-modal attention\n",
    "\n",
    "This model processes the individual modalities through different pathways. Then, it uses cross-modal attention layers to study the dependencies between text and video, and text and audio, prioritising text as the most important modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(BaseModel):\n",
    "    def __init__(self, num_classes, d_model, num_heads):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        \n",
    "        # Text feature processing using bi-LSTM\n",
    "        self.text_mask = Masking(mask_value=0.0, input_shape=(None, 110))\n",
    "        self.text_bilstm_1 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        self.text_bilstm_2 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        self.text_dense = Dense(d_model, activation='relu')\n",
    "\n",
    "        # Audio feature processing using 1D CNN with padding\n",
    "        self.audio_mask = Masking(mask_value=0.0, input_shape=(None, 110))\n",
    "        self.audio_cnn_1 = Conv1D(filters=128, kernel_size=8, activation='relu', padding='same')\n",
    "        self.audio_cnn_2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.audio_dense = Dense(d_model, activation='relu')\n",
    "\n",
    "        # Video feature processing using CNN and Multi-Head Attention\n",
    "        self.video_mask = Masking(mask_value=0.0, input_shape=(None, 110))\n",
    "        self.video_attn = MultiHeadAttention(num_heads=8, key_dim=64)\n",
    "        self.video_cnn_1 = Conv1D(128, kernel_size=8, activation='relu', padding='same')\n",
    "        self.video_cnn_2 = Conv1D(128, kernel_size=3, activation='relu', padding='same')\n",
    "        self.video_dense = Dense(d_model, activation='relu')\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Final processing and classification layers\n",
    "        self.concat = Concatenate()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        audio_features, text_features, video_features = inputs\n",
    "        \n",
    "        # Process video features and reduce dimensionality\n",
    "        video_features = self.video_mask(video_features)\n",
    "        video_features = self.video_attn(video_features, video_features)\n",
    "        video_features = self.video_cnn_1(video_features)\n",
    "        video_features = self.video_cnn_2(video_features)\n",
    "        video_features = self.video_dense(video_features)\n",
    "\n",
    "        # Process text features\n",
    "        text_features = self.text_mask(text_features)\n",
    "        text_features = self.text_bilstm_1(text_features)\n",
    "        text_features = self.text_bilstm_2(text_features)\n",
    "        text_features = self.text_dense(text_features)\n",
    "\n",
    "        # Process audio features\n",
    "        audio_features = self.audio_mask(audio_features)\n",
    "        audio_features = self.audio_cnn_1(audio_features)\n",
    "        audio_features = self.audio_cnn_2(audio_features)\n",
    "        audio_features = self.audio_dense(audio_features)\n",
    "        \n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(text_features, audio_features)\n",
    "        text_video_attn = self.text_video_attention(text_features, video_features)\n",
    "        \n",
    "        # Combine features from both attention mechanisms\n",
    "        combined_features = self.concat([text_features, text_audio_attn, text_video_attn])\n",
    "        outputs = self.classifier(combined_features)\n",
    "\n",
    "        # Return the final prediction\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/venessa/Desktop/5342project/venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/var/folders/6l/xwrqvy7n1814gqhygtglkblw0000gn/T/ipykernel_93552/792049276.py\", line 2, in None  *\n        lambda y_true, y_pred: masked_categorical_crossentropy(y_true, y_pred, train_mask)\n    File \"/Users/venessa/Desktop/5342project/model_utils.py\", line 15, in masked_categorical_crossentropy  *\n        loss *= mask\n\n    ValueError: Dimensions must be equal, but are 32 and 120 for '{{node lambda/mul}} = Mul[T=DT_FLOAT](lambda/softmax_cross_entropy_with_logits/Reshape_2, lambda/mul/y)' with input shapes: [32,110], [120,110].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m multimodal_model \u001b[38;5;241m=\u001b[39m MultimodalModel(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, d_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      2\u001b[0m multimodal_model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: masked_categorical_crossentropy(y_true, y_pred, train_mask), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmultimodal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_audio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_visual\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m multimodal_model\u001b[38;5;241m.\u001b[39mevaluate([test_audio, test_text, test_visual], test_label, test_mask)\n\u001b[1;32m      5\u001b[0m multimodal_model\u001b[38;5;241m.\u001b[39mprint_metrics()\n",
      "File \u001b[0;32m~/Desktop/5342project/venv/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/6l/xwrqvy7n1814gqhygtglkblw0000gn/T/__autograph_generated_file1r5seuto.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/6l/xwrqvy7n1814gqhygtglkblw0000gn/T/__autograph_generated_file79gvrck2.py:5\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 5\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_function_scope\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_categorical_crossentropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlscope\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSTD\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m/var/folders/6l/xwrqvy7n1814gqhygtglkblw0000gn/T/__autograph_generated_file79gvrck2.py:5\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.<lambda>\u001b[0;34m(lscope)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_factory\u001b[39m(ag__):\n\u001b[0;32m----> 5\u001b[0m     tf__lam \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m y_true, y_pred: ag__\u001b[38;5;241m.\u001b[39mwith_function_scope((\u001b[38;5;28;01mlambda\u001b[39;00m lscope: \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_categorical_crossentropy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlscope\u001b[49m\u001b[43m)\u001b[49m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlscope\u001b[39m\u001b[38;5;124m'\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mSTD))\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf__lam\n",
      "File \u001b[0;32m/var/folders/6l/xwrqvy7n1814gqhygtglkblw0000gn/T/__autograph_generated_filel84_js18.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__masked_categorical_crossentropy\u001b[0;34m(y_true, y_pred, mask)\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mcategorical_crossentropy, (ag__\u001b[38;5;241m.\u001b[39mld(y_true), ag__\u001b[38;5;241m.\u001b[39mld(y_pred)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(loss)\n\u001b[0;32m---> 12\u001b[0m loss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/venessa/Desktop/5342project/venv/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/var/folders/6l/xwrqvy7n1814gqhygtglkblw0000gn/T/ipykernel_93552/792049276.py\", line 2, in None  *\n        lambda y_true, y_pred: masked_categorical_crossentropy(y_true, y_pred, train_mask)\n    File \"/Users/venessa/Desktop/5342project/model_utils.py\", line 15, in masked_categorical_crossentropy  *\n        loss *= mask\n\n    ValueError: Dimensions must be equal, but are 32 and 120 for '{{node lambda/mul}} = Mul[T=DT_FLOAT](lambda/softmax_cross_entropy_with_logits/Reshape_2, lambda/mul/y)' with input shapes: [32,110], [120,110].\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "multimodal_model = MultimodalModel(num_classes=5, d_model=100, num_heads=8)\n",
    "\n",
    "# Compile the model\n",
    "multimodal_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodal_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodal_model.evaluate([test_audio, test_text, test_visual], test_label)\n",
    "\n",
    "# Print the metrics\n",
    "multimodal_model.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Model with Self-Attention\n",
    "\n",
    "This model first adds a layer of self-attention for each modality, before processing the individual modalities through different pathways. Then, similar to above, it uses cross-modal attention layers to study the dependencies between text and video, and text and audio, prioritising text as the most important modality.\n",
    "\n",
    "It was found that the added layer of attention worsened performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalAttentionModel(BaseModel):\n",
    "    def __init__(self, num_classes, d_model=128, num_heads=4):\n",
    "        super(MultimodalAttentionModel, self).__init__()\n",
    "        \n",
    "        # Text Pathway - Bi-LSTM with Attention\n",
    "        self.text_mask = Masking(mask_value=0.0, input_shape=(None, 110))\n",
    "        self.text_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.text_bi_lstm1 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        self.text_bi_lstm2 = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        self.text_dropout = Dropout(0.5)\n",
    "        self.text_dense = Dense(100, activation='relu')\n",
    "        \n",
    "        # Audio Pathway - CNN with Attention\n",
    "        self.audio_mask = Masking(mask_value=0.0, input_shape=(None, 110))\n",
    "        self.audio_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.audio_conv1 = Conv1D(100, kernel_size=10, activation='relu', padding='same')\n",
    "        self.audio_conv2 = Conv1D(100, kernel_size=9, activation='relu', padding='same')\n",
    "        self.audio_conv3 = Conv1D(100, kernel_size=8, activation='relu', padding='same')\n",
    "        self.audio_dense = Dense(100, activation='relu')\n",
    "        \n",
    "        # Video Pathway - Similar to Audio\n",
    "        self.video_mask = Masking(mask_value=0.0, input_shape=(None, 110))\n",
    "        self.video_attention = MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "        self.video_conv1 = Conv1D(100, kernel_size=10, activation='relu', padding='same')\n",
    "        self.video_conv2 = Conv1D(100, kernel_size=9, activation='relu', padding='same')\n",
    "        self.video_conv3 = Conv1D(100, kernel_size=8, activation='relu', padding='same')\n",
    "        self.video_dense = Dense(100, activation='relu')\n",
    "        \n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Final processing and classification layers\n",
    "        self.concat = Concatenate()\n",
    "        self.global_pool = GlobalAveragePooling1D()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        text_inputs, audio_inputs, video_inputs = inputs\n",
    "        \n",
    "        # Text Pathway\n",
    "        x_text = self.text_mask(text_inputs)\n",
    "        x_text = self.text_attention(x_text, x_text)\n",
    "        x_text = self.text_bi_lstm1(x_text)\n",
    "        x_text = self.text_bi_lstm2(x_text)\n",
    "        x_text = self.text_dropout(x_text)\n",
    "        x_text = self.text_dense(x_text)\n",
    "        \n",
    "        # Audio Pathway\n",
    "        x_audio = self.audio_mask(audio_inputs)\n",
    "        x_audio = self.audio_attention(x_audio, x_audio)\n",
    "        x_audio = self.audio_conv1(x_audio)\n",
    "        x_audio = self.audio_conv2(x_audio)\n",
    "        x_audio = self.audio_conv3(x_audio)\n",
    "        x_audio = self.audio_dense(x_audio)\n",
    "        \n",
    "        # Video Pathway\n",
    "        x_video = self.video_mask(video_inputs)\n",
    "        x_video = self.video_attention(x_video, x_video)\n",
    "        x_video = self.video_conv1(x_video)\n",
    "        x_video = self.video_conv2(x_video)\n",
    "        x_video = self.video_conv3(x_video)\n",
    "        x_video = self.video_dense(x_video)\n",
    "\n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(x_text, x_audio)\n",
    "        text_video_attn = self.text_video_attention(x_text, x_video)\n",
    "        \n",
    "        # Combine features from both attention mechanisms\n",
    "        combined_features = self.concat([x_text, text_audio_attn, text_video_attn])\n",
    "        outputs = self.classifier(combined_features)\n",
    "\n",
    "        # Return the final prediction\n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "multimodal_attention_model = MultimodalAttentionModel(num_classes=5)\n",
    "\n",
    "# Compile the model\n",
    "multimodal_attention_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodal_attention_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodal_attention_model.evaluate([test_audio, test_text, test_visual], test_label)\n",
    "\n",
    "# Print the metrics\n",
    "multimodal_attention_model.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Model with simplified preprocessing\n",
    "This model applies less processing to the individual modalities. Video features are processed through a transformer block and their dimensionality is reduced. Again, cross-modal attention is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformerModel(Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, num_classes=10):\n",
    "        super(MultimodalTransformerModel, self).__init__()\n",
    "\n",
    "        # Video feature processing using Transformer block\n",
    "        self.video_transformer = TransformerBlock(512, num_heads, dff, rate)\n",
    "        self.video_dense = Dense(64)\n",
    "        \n",
    "        # Text feature processing using bi-LSTM\n",
    "        self.text_lstm = Bidirectional(LSTM(64, return_sequences=True))\n",
    "        self.text_dense = Dense(64, activation='relu')\n",
    "\n",
    "        # Audio feature processing using 1D CNN with padding\n",
    "        self.audio_cnn = Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')\n",
    "        self.audio_dense = Dense(64, activation='relu')\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Final processing and classification layers\n",
    "        self.concat = Concatenate()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        audio_features, text_features, video_features = inputs\n",
    "        \n",
    "        # Process video features and reduce dimensionality\n",
    "        video_features = self.video_transformer(video_features, training=training)\n",
    "        video_features = self.video_dense(video_features)\n",
    "\n",
    "        # Process text features\n",
    "        text_features = self.text_lstm(text_features)\n",
    "        text_features = self.text_dense(text_features)\n",
    "\n",
    "        # Process audio features\n",
    "        audio_features = self.audio_cnn(audio_features)\n",
    "        audio_features = self.audio_dense(audio_features)\n",
    "        \n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(text_features, audio_features)\n",
    "        text_video_attn = self.text_video_attention(text_features, video_features)\n",
    "        \n",
    "        # Combine features from both attention mechanisms\n",
    "        combined_features = self.concat([text_features, text_audio_attn, text_video_attn])\n",
    "        outputs = self.classifier(combined_features)\n",
    "\n",
    "        # Return the final prediction\n",
    "        return outputs\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        self.test_metrics =  {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'cm': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "    def print_metrics(self):\n",
    "        print(\"Test Metrics:\")\n",
    "        print(f\"Accuracy: {self.test_metrics['accuracy']:.4f}\")\n",
    "        # print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        # print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {self.test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "        class_labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(self.test_metrics['cm'], annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "multimodal_transformer_model = MultimodalTransformerModel(d_model=100, num_heads=4, dff=2048, rate=0.1, num_classes=6)\n",
    "\n",
    "# Compile the model\n",
    "multimodal_transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodal_transformer_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodal_transformer_model.evaluate([test_audio, test_text, test_visual], test_label)\n",
    "\n",
    "# Print the metrics\n",
    "multimodal_transformer_model.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalTransformerModel(Model):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1, num_classes=10):\n",
    "        super(MultimodalTransformerModel, self).__init__()\n",
    "        # Transformer block for processing video features\n",
    "        self.video_transformer = TransformerBlock(d_model=512, num_heads=8, dff=dff, rate=rate)\n",
    "        self.video_dense = Dense(d_model)\n",
    "        \n",
    "        # Text feature processing using bi-LSTM\n",
    "        self.text_transformer = TransformerBlock(d_model=100, num_heads=4, dff=dff, rate=rate)\n",
    "        self.text_dense = Dense(d_model)\n",
    "\n",
    "        # Audio feature processing using 1D CNN with padding\n",
    "        self.audio_transformer = TransformerBlock(d_model=100, num_heads=4, dff=dff, rate=rate)\n",
    "        self.audio_dense = Dense(d_model)\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Final processing and classification layers\n",
    "        self.concat = Concatenate()\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        audio_features, text_features, video_features = inputs\n",
    "        \n",
    "        # Process video features and reduce dimensionality\n",
    "        video_features = self.video_transformer(video_features, training=training)\n",
    "        video_features = self.video_dense(video_features)\n",
    "\n",
    "        # Process text features\n",
    "        text_features = self.text_transformer(text_features, training=training)\n",
    "        text_features = self.text_dense(text_features)\n",
    "\n",
    "        # Process audio features\n",
    "        audio_features = self.audio_transformer(audio_features, training=training)\n",
    "        audio_features = self.audio_dense(audio_features)\n",
    "        \n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(text_features, audio_features)\n",
    "        text_video_attn = self.text_video_attention(text_features, video_features)\n",
    "        \n",
    "        # Combine features from both attention mechanisms\n",
    "        combined_features = self.concat([text_features, text_audio_attn, text_video_attn])\n",
    "        outputs = self.classifier(combined_features)\n",
    "\n",
    "        # Return the final prediction\n",
    "        return outputs\n",
    "    \n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        self.test_metrics =  {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'cm': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "\n",
    "    def print_metrics(self):\n",
    "        print(\"Test Metrics:\")\n",
    "        print(f\"Accuracy: {self.test_metrics['accuracy']:.4f}\")\n",
    "        # print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        # print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {self.test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "        class_labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(self.test_metrics['cm'], annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "multimodal_transformer_model = MultimodalTransformerModel(num_classes=6, d_model=100, num_heads=4, dff=2048, rate=0.1)\n",
    "\n",
    "# Compile the model\n",
    "multimodal_transformer_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodal_transformer_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodal_transformer_model.evaluate([test_audio, test_text, test_visual], test_label)\n",
    "\n",
    "# Print the metrics\n",
    "multimodal_transformer_model.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIMNet Multimodal Model\n",
    "\n",
    "The models below incorporate TIMNet as part of their architecture. Different architectures are explored to find the optimal one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAwareBlock(Layer):\n",
    "    def __init__(self, nb_filters, kernel_size, dilation_rate, activation, dropout_rate, name=''):\n",
    "        super(TemporalAwareBlock, self).__init__(name=name)\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Primary layers used in each temporal block\n",
    "        self.conv1 = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')\n",
    "        self.norm1 = BatchNormalization()\n",
    "        self.act1 = Activation(activation)\n",
    "        self.dropout1 = SpatialDropout1D(dropout_rate)\n",
    "\n",
    "        self.conv2 = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')\n",
    "        self.norm2 = BatchNormalization()\n",
    "        self.act2 = Activation(activation)\n",
    "        self.dropout2 = SpatialDropout1D(dropout_rate)\n",
    "        \n",
    "        # Additional Conv1D layer for matching dimensions if needed\n",
    "        self.dim_match_conv = Conv1D(filters=nb_filters, kernel_size=1, padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        original_x = inputs\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.norm1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x)\n",
    "        x = self.act2(x)\n",
    "        output_2_1 = self.dropout2(x)\n",
    "\n",
    "        # Check and apply dimension matching if necessary\n",
    "        if original_x.shape[-1] != output_2_1.shape[-1]:\n",
    "            original_x = self.dim_match_conv(original_x)\n",
    "\n",
    "        # Applying sigmoid and element-wise multiplication\n",
    "        output_2_1 = Lambda(sigmoid)(output_2_1)\n",
    "        F_x = Lambda(lambda x: tf.multiply(x[0], x[1]))([original_x, output_2_1])\n",
    "        return F_x\n",
    "\n",
    "\n",
    "class TIMNET(Model):\n",
    "    def __init__(self, nb_filters=64, kernel_size=2, nb_stacks=1, dilations=None, activation=\"relu\", dropout_rate=0.1, return_sequences=True, name='TIMNET'):\n",
    "        super(TIMNET, self).__init__(name=name)\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.dilations = dilations or [1, 2, 4, 8]\n",
    "        self.forward_blocks = []\n",
    "        self.backward_blocks = []\n",
    "\n",
    "        for _ in range(nb_stacks):\n",
    "            for dilation in self.dilations:\n",
    "                self.forward_blocks.append(TemporalAwareBlock(nb_filters, kernel_size, dilation, activation, dropout_rate))\n",
    "                self.backward_blocks.append(TemporalAwareBlock(nb_filters, kernel_size, dilation, activation, dropout_rate))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        forward = inputs\n",
    "        backward = tf.reverse(inputs, axis=[1])  # Assuming the time dimension is axis 1\n",
    "\n",
    "        final_skip_connections = []\n",
    "\n",
    "        for forward_block, backward_block in zip(self.forward_blocks, self.backward_blocks):\n",
    "            skip_out_forward = forward_block(forward)\n",
    "            skip_out_backward = backward_block(backward)\n",
    "            # Combine skip outputs from forward and backward blocks at the same level\n",
    "            temp_skip = Add()([skip_out_forward, skip_out_backward])\n",
    "            final_skip_connections.append(temp_skip)\n",
    "\n",
    "        # Combine all skip connections\n",
    "        if final_skip_connections:\n",
    "            output = final_skip_connections[0]\n",
    "            for skip_connection in final_skip_connections[1:]:\n",
    "                output = concatenate([output, skip_connection], axis=-1)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using TIMNet on already combined features, similar to initial run of TIMNet. The results are worse than the original TIMNet, but that could be due to less epochs, or other factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalNet(Model):\n",
    "    def __init__(self, num_classes=6, d_model=128, num_heads=4):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "        # Define pathways for text, audio, and video\n",
    "        self.timnet = TIMNET(nb_filters=64, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4, 8], activation=\"relu\", dropout_rate=0.2)\n",
    "        self.concat = Concatenate()\n",
    "        self.classifier = TimeDistributed(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        timnet = self.timnet(inputs)\n",
    "\n",
    "        outputs = self.classifier(timnet)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        self.test_metrics =  {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'cm': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    def print_metrics(self):\n",
    "        print(\"Test Metrics:\")\n",
    "        print(f\"Accuracy: {self.test_metrics['accuracy']:.4f}\")\n",
    "        # print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        # print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {self.test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "        class_labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(self.test_metrics['cm'], annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "multimodalnet_model = MultiModalNet(num_classes=6)\n",
    "\n",
    "# Compile the model\n",
    "multimodalnet_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodalnet_model.fit(train_data, train_label, epochs=50, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodalnet_model.evaluate(test_data, test_label)\n",
    "\n",
    "# Print the metrics\n",
    "multimodalnet_model.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using TIMNet to preprocess individual modalities before applying cross-modal attention. Very slight improvement from the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalNet(Model):\n",
    "    def __init__(self, num_classes=6, d_model=128, num_heads=4):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "        # Define pathways for text, audio, and video\n",
    "        self.text_timnet = TIMNET(nb_filters=64, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4], activation=\"relu\", dropout_rate=0.2)\n",
    "        self.audio_timnet = TIMNET(nb_filters=64, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4], activation=\"relu\", dropout_rate=0.2)\n",
    "        self.video_timnet = TIMNET(nb_filters=64, kernel_size=3, nb_stacks=2, dilations=[1, 2, 4], activation=\"relu\", dropout_rate=0.2)\n",
    "        \n",
    "        # Cross modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        \n",
    "        self.concat = Concatenate()\n",
    "        self.classifier = TimeDistributed(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        text_inputs, audio_inputs, video_inputs = inputs\n",
    "\n",
    "        x_text = text_inputs\n",
    "        x_audio = audio_inputs\n",
    "        x_video = video_inputs\n",
    "\n",
    "        # TIMNET processing\n",
    "        x_text = self.text_timnet(x_text)\n",
    "        x_audio = self.audio_timnet(x_audio)\n",
    "        x_video = self.video_timnet(x_video)\n",
    "\n",
    "        # Apply cross-modal attention between audio-video and text-video\n",
    "        text_audio_attn = self.text_audio_attention(x_text, x_audio)\n",
    "        text_video_attn = self.text_video_attention(x_text, x_video)\n",
    "\n",
    "        # Combine and classify\n",
    "        combined_features = self.concat([x_text, text_audio_attn, text_video_attn])\n",
    "        outputs = self.classifier(combined_features)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        self.test_metrics =  {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'cm': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    def print_metrics(self):\n",
    "        print(\"Test Metrics:\")\n",
    "        print(f\"Accuracy: {self.test_metrics['accuracy']:.4f}\")\n",
    "        # print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        # print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {self.test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "        class_labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(self.test_metrics['cm'], annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "multimodalnet_model = MultiModalNet(num_classes=6)\n",
    "\n",
    "# Compile the model\n",
    "multimodalnet_model .compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodalnet_model .fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodalnet_model .evaluate([test_audio, test_text, test_visual], test_label)\n",
    "\n",
    "# Print the metrics\n",
    "multimodalnet_model.print_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Processing each modality with individual pathways before cross-modal attention, and finally TIMNet on concatenated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalNet(Model):\n",
    "    def __init__(self, num_classes=6, d_model=128, num_heads=4, nb_filters=64, kernel_size=3, nb_stacks=1, dilations=[1, 2, 4, 8]):\n",
    "        super(MultiModalNet, self).__init__()\n",
    "        # Initialize processing layers for each modality\n",
    "        self.text_lstm = Bidirectional(LSTM(50, return_sequences=True))\n",
    "        self.audio_cnn = Conv1D(filters=100, kernel_size=3, activation='relu', padding='same')\n",
    "        self.video_cnn = Conv1D(filters=100, kernel_size=3, activation='relu', padding='same')\n",
    "\n",
    "        # Cross-modal attention layers\n",
    "        self.text_audio_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "        self.text_video_attention = CrossModalAttentionLayer(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Concatenation layer\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "        # TIMNET applied after cross-modal attention\n",
    "        self.timnet = TIMNET(nb_filters=nb_filters, kernel_size=kernel_size, nb_stacks=nb_stacks, dilations=dilations, activation=\"relu\", dropout_rate=0.1)\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = Dense(num_classes, activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        text_inputs, audio_inputs, video_inputs = inputs\n",
    "\n",
    "        # Process each modality\n",
    "        x_text = self.text_lstm(text_inputs)\n",
    "        x_audio = self.audio_cnn(audio_inputs)\n",
    "        x_video = self.video_cnn(video_inputs)\n",
    "\n",
    "        # Apply cross-modal attention\n",
    "        text_audio_attn = self.text_audio_attention(x_text, x_audio)\n",
    "        text_video_attn = self.text_video_attention(x_text, x_video)\n",
    "\n",
    "        # Combine features from attention mechanisms\n",
    "        combined_features = self.concat([x_text, text_audio_attn, text_video_attn])\n",
    "\n",
    "        # Pass combined features through TIMNET\n",
    "        processed_features = self.timnet(combined_features)\n",
    "\n",
    "        # Pool and classify\n",
    "        outputs = self.classifier(processed_features)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        predictions = self.predict(x_test)\n",
    "        y_pred = np.argmax(predictions, axis=-1).flatten()\n",
    "        y_true = np.argmax(y_test, axis=-1).flatten()\n",
    "\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "\n",
    "        self.test_metrics =  {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'cm': confusion_matrix(y_true, y_pred)\n",
    "        }\n",
    "    \n",
    "    def print_metrics(self):\n",
    "        print(\"Test Metrics:\")\n",
    "        print(f\"Accuracy: {self.test_metrics['accuracy']:.4f}\")\n",
    "        # print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "        # print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {self.test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "        class_labels = ['hap', 'sad', 'neu', 'ang', 'exc', 'fru']\n",
    "        print(\"Confusion Matrix:\")\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sns.heatmap(self.test_metrics['cm'], annot=True, fmt='g', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n",
    "        plt.xlabel('Predicted labels')\n",
    "        plt.ylabel('True labels')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "multimodalnet_model = MultiModalNet(num_classes=6)\n",
    "\n",
    "# Compile the model\n",
    "multimodalnet_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "multimodalnet_model.fit([train_audio, train_text, train_visual], train_label, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "multimodalnet_model.evaluate([test_audio, test_text, test_visual], test_label)\n",
    "\n",
    "# Print the metrics\n",
    "multimodalnet_model.print_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
